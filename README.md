# Project Physics-AGI

## âœ… ç‹€æ…‹ï¼šå®Œå…¨å°±ç·’ | è¨“ç·´è…³æœ¬å¯é‹è¡Œ

**æœ€æ–°æ›´æ–°** (2026-01-09):
- âœ… ä¿®å¾©æ‰€æœ‰ä¾è³´å•é¡Œ
- âœ… é…ç½®ç„¡é ­æ¸²æŸ“ï¼ˆEGLï¼‰
- âœ… è¨“ç·´è…³æœ¬å®Œå…¨å¯ç”¨
- âœ… æ‰€æœ‰ 5/5 æ¨¡çµ„æ¸¬è©¦é€šé

---

## ğŸ§  ç³»çµ±æ¶æ§‹ï¼šModel-Based Reinforcement Learning (MBRL)

é€™æ˜¯ä¸€å€‹å®Œæ•´çš„ World Model å¯¦ç¾,æ¡ç”¨ **ç‰©ç†æ„ŸçŸ¥ AI** æ¶æ§‹ï¼Œèƒ½å¤ ï¼š
- ğŸ¯ **ç†è§£ç‰©ç†æ³•å‰‡**ï¼ˆè€Œéæ­»è¨˜ç¡¬èƒŒï¼‰
- ğŸ”® **åœ¨è…¦æµ·ä¸­æƒ³åƒæœªä¾†**ï¼ˆå…§éƒ¨æ¨¡æ“¬ï¼‰
- ğŸ® **åœ¨è™›æ“¬ç’°å¢ƒä¸­è¦åŠƒ**ï¼ˆé›¶æ¨£æœ¬é·ç§»ï¼‰

---

## ğŸ“‹ ç³»çµ±æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ç‰©ç†æ„ŸçŸ¥å±¤                              â”‚
â”‚  Variational Encoder: o_t â†’ z_t (å£“ç¸® + å»å™ª)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ z_t (æ½›åœ¨ç‰¹å¾µ)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ä¸–ç•Œæ¨¡æ“¬å±¤ (RSSM)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Prior Path:  h_{t-1}, a_{t-1} â†’ z_t (æƒ³åƒ)  â”‚       â”‚
â”‚  â”‚ Posterior Path: h_{t-1}, o_t â†’ z_t (æ ¡æº–)    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚            â†“ KL(Prior || Posterior) â†’ 0                 â”‚
â”‚       (ç•¶é€™å€‹è¶¨è¿‘0æ™‚ï¼ŒAIç†è§£äº†ç‰©ç†)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ (h_t, z_t) - å…§éƒ¨ç‹€æ…‹
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ä»£ç†æ§åˆ¶å±¤                              â”‚
â”‚  Reward Model:  r_t = R(s_t)                            â”‚
â”‚  Value Model:   V(s_t) = E[Î£ Î³^k r_{t+k}]              â”‚
â”‚  Actor:         Ï€(a|s) - ç­–ç•¥ç¶²çµ¡                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´

```bash
# Clone repository
git clone https://github.com/RainesTaiwan/Project-Physics-AGI.git
cd Project-Physics-AGI

# Create conda environment
conda create -n physics-agi python=3.10
conda activate physics-agi

# Install dependencies
pip install -r requirements.txt

# Install MuJoCo (for physics simulation)
pip install mujoco dm-control
```

### 2. è¨“ç·´æ¨¡å‹

```bash
# Train on DMControl Walker-Walk task
python train.py --config config.yaml --steps 1000000

# Train with custom config
python train.py --config my_config.yaml --steps 500000

# Resume from checkpoint
python train.py --config config.yaml --checkpoint logs/physics_agi_v1/checkpoints/checkpoint_100000.pt
```

### 3. è©•ä¼°æ¨¡å‹

```bash
# Evaluate trained model
python evaluate.py \
    --config config.yaml \
    --checkpoint logs/physics_agi_v1/checkpoints/checkpoint_1000000.pt \
    --episodes 10 \
    --deterministic
```

### 4. ç›£æ§è¨“ç·´

```bash
# Launch TensorBoard
tensorboard --logdir logs/

# Open browser: http://localhost:6006
```

---

## ğŸ¯ æ ¸å¿ƒæ¨¡çµ„è©³è§£

### æ¨¡çµ„ Aï¼šè®Šåˆ†æ„ŸçŸ¥ç·¨ç¢¼å™¨ (Variational Encoder)

**æª”æ¡ˆ**: [src/models/encoder.py](src/models/encoder.py)

**åŠŸèƒ½**ï¼š
- å°‡é«˜ç¶­åœ–åƒ (64Ã—64Ã—3) å£“ç¸®åˆ°ä½ç¶­æ½›åœ¨ç©ºé–“ (32ç¶­)
- **ä¿¡æ¯ç“¶é ¸ (Information Bottleneck)**ï¼šå¼·åˆ¶åªä¿ç•™ç‰©ç†å¿…è¦ä¿¡æ¯
- æ¦‚ç‡æ€§ç·¨ç¢¼ï¼šæ•æ‰æ¸¬é‡ä¸ç¢ºå®šæ€§

**æ•¸å­¸åŸç†**ï¼š
```
q(z|o) = N(Î¼_enc(o), Ïƒ_enc(o))
z ~ q(z|o)
```

### æ¨¡çµ„ Bï¼šå¾ªç’°ç‹€æ…‹ç©ºé–“æ¨¡å‹ (RSSM)

**æª”æ¡ˆ**: [src/models/rssm.py](src/models/rssm.py)

**åŠŸèƒ½**ï¼šç³»çµ±çš„ã€Œç‰©ç†å¼•æ“ã€

**é›™è·¯å¾‘è¨­è¨ˆ**ï¼š
1. **Prior (æƒ³åƒè·¯å¾‘)**ï¼š`p(z_t | h_t)` - ç´”æ†‘å…§éƒ¨å‹•åŠ›å­¸é æ¸¬
2. **Posterior (çœŸå¯¦è·¯å¾‘)**ï¼š`q(z_t | h_t, o_t)` - çµåˆè§€æ¸¬æ ¡æº–

**ç‰©ç†ç†è§£æŒ‡æ¨™**ï¼š
```
KL(q(z_t|h_t,o_t) || p(z_t|h_t)) â†’ 0
```
ç•¶é€™å€‹å€¼è¶¨è¿‘ 0 æ™‚ï¼Œä»£è¡¨ AI èƒ½æº–ç¢ºé æ¸¬ç‰©ç†ç¾è±¡

### æ¨¡çµ„ C & Dï¼šActor-Critic ç³»çµ±

**æª”æ¡ˆ**: [src/models/actor_critic.py](src/models/actor_critic.py)

**çµ„ä»¶**ï¼š
- **Reward Model**: é æ¸¬å³æ™‚çå‹µ `r_t`
- **Value Model**: è©•ä¼°é•·æœŸåƒ¹å€¼ `V(s_t)`
- **Actor**: è¼¸å‡ºå‹•ä½œç­–ç•¥ `Ï€(a|s)`

---

## ğŸ“Š è¨“ç·´æµç¨‹

### éšæ®µ 1ï¼šDynamics Learning (å­¸ç¿’ç‰©ç†)

```python
Loss = Î»_recon * ||o_t - Ã´_t||Â² 
     + Î»_kl * KL(Posterior || Prior)
     + Î»_reward * ||r_t - rÌ‚_t||Â²
```

**ç›®æ¨™**ï¼š
1. é‡å»ºè§€æ¸¬ (è­‰æ˜æ²’æœ‰ä¸Ÿå¤±ä¿¡æ¯)
2. æœ€å°åŒ– KL æ•£åº¦ (å­¸ç¿’ç‰©ç†æ³•å‰‡)
3. é æ¸¬çå‹µ (ä»»å‹™ç›¸é—œ)

### éšæ®µ 2ï¼šBehavior Learning (å­¸ç¿’ç­–ç•¥)

åœ¨ **æƒ³åƒç©ºé–“ (Latent Space)** ä¸­å±•é–‹ 15 æ­¥è»Œè·¡ï¼š

```python
# Imagine rollout
for t in range(imagination_horizon):
    a_t ~ Ï€(Â·|s_t)           # Sample action
    s_{t+1} ~ p(Â·|s_t, a_t)  # Predict next state (dreaming)
    r_t = R(s_t)             # Predict reward
```

**å„ªå‹¢**ï¼š
- âœ… ä¸éœ€è¦åœ¨çœŸå¯¦ç’°å¢ƒä¸­è©¦éŒ¯
- âœ… å¯ä»¥å¿«é€Ÿè¦åŠƒï¼ˆ15 æ­¥åªéœ€ <1msï¼‰
- âœ… å®‰å…¨æ¢ç´¢ï¼ˆä¸æœƒæå£ç¡¬ä»¶ï¼‰

---

## ğŸ”¬ é—œéµæ•¸å­¸ç´„æŸ

### 1. ä¿¡æ¯ç“¶é ¸ (Information Bottleneck)

```python
latent_dim << input_dim
32 << (64 Ã— 64 Ã— 3) = 12288
å£“ç¸®æ¯”: 384å€
```

**ä½œç”¨**ï¼šè¿«ä½¿ AI å­¸æœƒã€ŒæŠ½è±¡ã€å’Œã€Œæ³›åŒ–ã€ï¼Œè€Œéè¨˜æ†¶

### 2. å¤šæ­¥é æ¸¬ä¸€è‡´æ€§ (Long-term Consistency)

```python
# Latent Overshooting
for k in range(overshooting_distance):
    z_{t+k} = RSSM.imagine_step(z_t, a_{t:t+k})
    loss += ||z_{t+k} - z_{t+k}^{real}||Â²
```

**ä½œç”¨**ï¼šç¢ºä¿é•·æœŸé æ¸¬ä¸æœƒç™¼æ•£

### 3. KL å¹³è¡¡ (KL Balancing)

```python
KL_loss = Î± * KL(Post || Prior) + (1-Î±) * KL(Prior || Post)
```

**ä½œç”¨**ï¼šé˜²æ­¢å¾Œé©—å´©å¡Œ (Posterior Collapse)

---

## ğŸ“‚ é …ç›®çµæ§‹

```
Project-Physics-AGI/
â”œâ”€â”€ config.yaml              # ç³»çµ±é…ç½®
â”œâ”€â”€ requirements.txt         # Python ä¾è³´
â”œâ”€â”€ train.py                 # è¨“ç·´è…³æœ¬
â”œâ”€â”€ evaluate.py              # è©•ä¼°è…³æœ¬
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoder.py       # æ¨¡çµ„ A: ç·¨ç¢¼å™¨/è§£ç¢¼å™¨
â”‚   â”‚   â”œâ”€â”€ rssm.py          # æ¨¡çµ„ B: RSSM
â”‚   â”‚   â””â”€â”€ actor_critic.py  # æ¨¡çµ„ C/D: Actor-Critic
â”‚   â”œâ”€â”€ trainer.py           # è¨“ç·´å™¨
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ replay_buffer.py # ç¶“é©—å›æ”¾ç·©è¡å€
â”‚       â””â”€â”€ env_wrapper.py   # ç’°å¢ƒåŒ…è£å™¨
â””â”€â”€ logs/                    # è¨“ç·´æ—¥èªŒå’Œ checkpoints
```

---

## ğŸ® æ”¯æŒçš„ç’°å¢ƒ

### DeepMind Control Suite (æ¨è–¦)

```yaml
environment:
  name: "DMC-walker-walk"      # Walker: Walk
  # name: "DMC-cheetah-run"    # Cheetah: Run
  # name: "DMC-cartpole-swingup" # CartPole: Swing Up
  backend: "mujoco"
```

### OpenAI Gymnasium

```yaml
environment:
  name: "HalfCheetah-v4"
  # name: "Hopper-v4"
  # name: "Ant-v4"
```

---

## ğŸ”§ é…ç½®èªªæ˜

### é—œéµè¶…åƒæ•¸

```yaml
# Latent dimensions
encoder:
  latent_dim: 32              # éš¨æ©Ÿæ½›åœ¨è®Šé‡ç¶­åº¦
rssm:
  deterministic_size: 200     # ç¢ºå®šæ€§è¨˜æ†¶ç¶­åº¦

# KL divergence
rssm:
  free_nats: 3.0              # æœ€å° KL é–¾å€¼
  kl_balance_scale: 0.8       # KL å¹³è¡¡ä¿‚æ•¸

# Imagination
rssm:
  imagination_horizon: 15     # æƒ³åƒæœªä¾†æ­¥æ•¸

# Training
training:
  model_lr: 6e-4              # World Model å­¸ç¿’ç‡
  actor_lr: 8e-5              # Actor å­¸ç¿’ç‡
  sequence_length: 50         # è¨“ç·´åºåˆ—é•·åº¦
  batch_size: 50              # Batch å¤§å°
```

---

## ğŸ“ˆ å¯¦é©—çµæœè¿½è¹¤

### TensorBoard æŒ‡æ¨™

**Dynamics (ç‰©ç†å­¸ç¿’)**ï¼š
- `dynamics/reconstruction_loss` - é‡å»ºèª¤å·®
- `dynamics/kl_loss` - Prior/Posterior KL æ•£åº¦ (è¶Šä½ = è¶Šç†è§£ç‰©ç†)
- `dynamics/reward_loss` - çå‹µé æ¸¬èª¤å·®

**Behavior (ç­–ç•¥å­¸ç¿’)**ï¼š
- `behavior/actor_loss` - ç­–ç•¥æ¢¯åº¦æå¤±
- `behavior/value_loss` - åƒ¹å€¼å‡½æ•¸èª¤å·®
- `behavior/mean_return` - æƒ³åƒè»Œè·¡çš„å›å ±

**Collection (æ•¸æ“šæ”¶é›†)**ï¼š
- `collect/mean_episode_reward` - çœŸå¯¦ç’°å¢ƒå¹³å‡çå‹µ
- `collect/mean_episode_length` - å¹³å‡ episode é•·åº¦

---

## ğŸ§ª æ¸¬è©¦æ¨¡çµ„

æ¯å€‹æ ¸å¿ƒæ¨¡çµ„éƒ½æœ‰ç¨ç«‹æ¸¬è©¦ï¼š

```bash
# Test encoder
python -m src.models.encoder

# Test RSSM
python -m src.models.rssm

# Test actor-critic
python -m src.models.actor_critic

# Test replay buffer
python -m src.utils.replay_buffer

# Test environment
python -m src.utils.env_wrapper
```

---

## ğŸš§ å·²çŸ¥é™åˆ¶èˆ‡æœªä¾†å·¥ä½œ

### ç•¶å‰é™åˆ¶ï¼š
- âš ï¸ åƒ…æ”¯æŒé€£çºŒå‹•ä½œç©ºé–“ (é›¢æ•£å‹•ä½œéœ€è¦ä¿®æ”¹ Actor)
- âš ï¸ åœ–åƒè¼¸å…¥é™åˆ¶åœ¨ 64Ã—64 (æ›´é«˜åˆ†è¾¨ç‡éœ€è¦æ›´å¤§ç¶²çµ¡)
- âš ï¸ å–®æ™ºèƒ½é«”ç³»çµ± (å¤šæ™ºèƒ½é«”éœ€è¦æ“´å±•)

### æœªä¾†æ”¹é€²ï¼š
- ğŸ”œ æ”¯æŒ Vision Transformer (ViT) ç·¨ç¢¼å™¨
- ğŸ”œ æ•´åˆ Isaac Gym (GPU åŠ é€Ÿç‰©ç†)
- ğŸ”œ å¯¦ç¾ Dreamer v3 çš„æ”¹é€²
- ğŸ”œ å¤šæ¨¡æ…‹è¼¸å…¥ (è¦–è¦º + è§¸è¦º + æœ¬é«”æ„ŸçŸ¥)

---

## ğŸ“š åƒè€ƒæ–‡ç»

1. **DreamerV2**: Mastering Atari with Discrete World Models (Hafner et al., 2021)
2. **PlaNet**: A Deep Planning Network for Reinforcement Learning (Hafner et al., 2019)
3. **World Models**: Learning and Planning with Latent Dynamics (Ha & Schmidhuber, 2018)

---

## ğŸ“„ License

MIT License

---

**â­ å¦‚æœé€™å€‹é …ç›®å°ä½ æœ‰å¹«åŠ©ï¼Œè«‹çµ¦å€‹ Starï¼**